{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_git = ''\n",
    "root_wiki = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Code.data_visualization import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "def drop_from_interval(df: pd.DataFrame, min_interval: float, max_interval: float, n_samples_to_drop: int) -> pd.DataFrame:\n",
    "    tmp = df[df['a%'] >= min_interval]\n",
    "    if max_interval == 1:\n",
    "        tmp = tmp[tmp['a%'] <= max_interval]\n",
    "    else:\n",
    "        tmp = tmp[tmp['a%'] < max_interval]\n",
    "    to_drop_rows = tmp.sample(n_samples_to_drop)\n",
    "    to_drop = list(to_drop_rows.index)\n",
    "\n",
    "    return df.drop(to_drop), to_drop_rows\n",
    "\n",
    "def augment_interval(df_old: pd.DataFrame, df_for_augmentation: pd.DataFrame, min_interval: float, max_interval: float, n_samples_to_add: int) -> pd.DataFrame:\n",
    "    tmp = df_for_augmentation[df_for_augmentation['a%'] >= min_interval]\n",
    "    if max_interval == 1:\n",
    "        tmp = tmp[tmp['a%'] <= max_interval]\n",
    "    else:\n",
    "        tmp = tmp[tmp['a%'] < max_interval]\n",
    "    tmp = tmp.sample(n_samples_to_add)\n",
    "    result = pd.concat([df_old, tmp], axis=0)\n",
    "    return result\n",
    "\n",
    "def get_table_set(df: pd.DataFrame) -> set:\n",
    "    out = []\n",
    "    for r in tqdm(range(df.shape[0])):\n",
    "        out.append(df.iloc[r]['r_id'])\n",
    "        out.append(df.iloc[r]['s_id'])\n",
    "    return set(out)\n",
    "\n",
    "def check_no_repetitions(s1: set, s2: set) -> bool:\n",
    "    for t in tqdm(s1):\n",
    "        if t in s2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_couple_set(df: pd.DataFrame) -> set:\n",
    "    out = []\n",
    "    for r in range(df.shape[0]):\n",
    "        tmp1 = (df.iloc[r]['r_id'], df.iloc[r]['s_id'])\n",
    "        tmp2 = (df.iloc[r]['s_id'], df.iloc[r]['r_id'])\n",
    "        out.append(tmp1)\n",
    "        out.append(tmp2)\n",
    "    return set(out)\n",
    "\n",
    "def drop_duplicate_samples(pivot: pd.DataFrame, target: pd.DataFrame) -> pd.DataFrame:\n",
    "    print('Generating pivot couple set')\n",
    "    pivot_couple_set = get_couple_set(pivot)\n",
    "\n",
    "    print('Generating target couple set')\n",
    "    target_couple_set = get_couple_set(target)\n",
    "\n",
    "    samples_to_drop = []\n",
    "\n",
    "    print('Generating Samples to drop list')\n",
    "    for t in target_couple_set:\n",
    "        if t in pivot_couple_set:\n",
    "            samples_to_drop.append(t)\n",
    "    \n",
    "    samples_to_drop = set(samples_to_drop)\n",
    "\n",
    "    print('Generating rows to drop list')\n",
    "    rows_to_drop = []\n",
    "    for r in range(target.shape[0]):\n",
    "        tmp1 = (target.iloc[0]['r_id'], target.iloc[0]['s_id'])\n",
    "        tmp2 = (target.iloc[0]['s_id'], target.iloc[0]['r_id'])\n",
    "        if (tmp1 in samples_to_drop) or (tmp2 in samples_to_drop):\n",
    "            rows_to_drop.append(r)\n",
    "\n",
    "    print('Dropping rows')\n",
    "    out = target.drop(rows_to_drop)\n",
    "\n",
    "    return out\n",
    "\n",
    "def remove_exact_matches(df: pd.DataFrame):\n",
    "    out = df[df['a%']!=1]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing tuples for data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(root_git+'/balanced_datasets/lsh_candidates/train_stats_200000.csv')\n",
    "t2 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/train_stats_400000.csv')\n",
    "t3 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/train_stats_600000.csv')\n",
    "t4 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/train_stats_800000.csv')\n",
    "t5 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/train_stats_1000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([t1, t2, t3, t4, t5], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= result.drop('id', axis=1)\n",
    "result= result.drop('total_time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(root_git+'balanced_datasets/unbalanced_datasets/train_unbalanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(root_git+'balanced_datasets/lsh_candidates/test_stats_200000.csv').drop('id',axis=1).to_csv(root_git+'balanced_datasets/unbalanced_datasets/test_unbalanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(root_git+'balanced_datasets/lsh_candidates/valid_stats_200000.csv').drop('id',axis=1).to_csv(root_git+'balanced_datasets/unbalanced_datasets/valid_unbalanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "te1 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/test_stats_200000.csv')\n",
    "te2 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/test_stats_300000.csv')\n",
    "te3 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/test_stats_500000.csv')\n",
    "te4 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/test_stats_600000.csv')\n",
    "\n",
    "tetot = result = pd.concat([te1, te2, te3, te4], axis=0)\n",
    "\n",
    "result_te= tetot.drop('id', axis=1)\n",
    "result_te['a%'] = result_te['a%'].fillna(0)\n",
    "\n",
    "result_te.to_csv(root_git+'balanced_datasets/unbalanced_datasets/test_unbalanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttttest = pd.read_csv(root_git+'balanced_datasets/unbalanced_datasets/test_unbalanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/valid_stats_200000.csv')\n",
    "v2 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/valid_stats_300000.csv')\n",
    "v3 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/valid_stats_500000.csv')\n",
    "v4 = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/valid_stats_600000.csv')\n",
    "\n",
    "vtot = result = pd.concat([v1, v2, v3, v4], axis=0)\n",
    "\n",
    "result_v= vtot.drop('id', axis=1)\n",
    "result_v['a%'] = result_v['a%'].fillna(0)\n",
    "\n",
    "result_v.to_csv(root_git+'balanced_datasets/unbalanced_datasets/valid_unbalanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vvvvalid = pd.read_csv(root_git+'balanced_datasets/unbalanced_datasets/valid_unbalanced.csv')\n",
    "vvvvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unbalanced = pd.read_csv(root_git+'balanced_datasets/unbalanced_datasets/test_unbalanced.csv')\n",
    "test_unbalanced = test_unbalanced.drop('total_time', axis=1)\n",
    "test_unbalanced['a%'] = test_unbalanced['a%'].fillna(0)\n",
    "\n",
    "test_data_augm = pd.read_csv(root_git+'balanced_datasets/lsh_candidates/test_stats_300000.csv')\n",
    "test_data_augm = test_data_augm.drop('total_time', axis=1)\n",
    "test_data_augm = test_data_augm.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gittables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_from_interval(df: pd.DataFrame, min_interval: float, max_interval: float, n_samples_to_drop: int) -> pd.DataFrame:\n",
    "    tmp = df[df['a%'] >= min_interval]\n",
    "    if max_interval == 1:\n",
    "        tmp = tmp[tmp['a%'] <= max_interval]\n",
    "    else:\n",
    "        tmp = tmp[tmp['a%'] < max_interval]\n",
    "    to_drop_rows = tmp.sample(n_samples_to_drop)\n",
    "    to_drop = list(to_drop_rows.index)\n",
    "\n",
    "    return df.drop(to_drop), to_drop_rows\n",
    "\n",
    "def augment_interval(df_old: pd.DataFrame, df_for_augmentation: pd.DataFrame, min_interval: float, max_interval: float, n_samples_to_add: int) -> pd.DataFrame:\n",
    "    tmp = df_for_augmentation[df_for_augmentation['a%'] >= min_interval]\n",
    "    if max_interval == 1:\n",
    "        tmp = tmp[tmp['a%'] <= max_interval]\n",
    "    else:\n",
    "        tmp = tmp[tmp['a%'] < max_interval]\n",
    "    tmp = tmp.sample(n_samples_to_add)\n",
    "    result = pd.concat([df_old, tmp], axis=0)\n",
    "    return result\n",
    "\n",
    "def get_table_set(df: pd.DataFrame) -> set:\n",
    "    out = []\n",
    "    for r in tqdm(range(df.shape[0])):\n",
    "        out.append(df.iloc[r]['r_id'])\n",
    "        out.append(df.iloc[r]['s_id'])\n",
    "    return set(out)\n",
    "\n",
    "def check_no_repetitions(s1: set, s2: set) -> bool:\n",
    "    for t in tqdm(s1):\n",
    "        if t in s2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_couple_set(df: pd.DataFrame) -> set:\n",
    "    out = []\n",
    "    for r in range(df.shape[0]):\n",
    "        tmp1 = (df.iloc[r]['r_id'], df.iloc[r]['s_id'])\n",
    "        tmp2 = (df.iloc[r]['s_id'], df.iloc[r]['r_id'])\n",
    "        out.append(tmp1)\n",
    "        out.append(tmp2)\n",
    "    return set(out)\n",
    "\n",
    "def drop_duplicate_samples(pivot: pd.DataFrame, target: pd.DataFrame) -> pd.DataFrame:\n",
    "    print('Generating pivot couple set')\n",
    "    pivot_couple_set = get_couple_set(pivot)\n",
    "\n",
    "    print('Generating target couple set')\n",
    "    target_couple_set = get_couple_set(target)\n",
    "\n",
    "    samples_to_drop = []\n",
    "\n",
    "    print('Generating Samples to drop list')\n",
    "    for t in target_couple_set:\n",
    "        if t in pivot_couple_set:\n",
    "            samples_to_drop.append(t)\n",
    "    \n",
    "    samples_to_drop = set(samples_to_drop)\n",
    "\n",
    "    print('Generating rows to drop list')\n",
    "    rows_to_drop = []\n",
    "    for r in range(target.shape[0]):\n",
    "        tmp1 = (target.iloc[0]['r_id'], target.iloc[0]['s_id'])\n",
    "        tmp2 = (target.iloc[0]['s_id'], target.iloc[0]['r_id'])\n",
    "        if (tmp1 in samples_to_drop) or (tmp2 in samples_to_drop):\n",
    "            rows_to_drop.append(r)\n",
    "\n",
    "    print('Dropping rows')\n",
    "    out = target.drop(rows_to_drop)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tables = pd.read_csv(root_git+'balanced_datasets/unbalanced_datasets/train_unbalanced_not_augmented.csv')\n",
    "test_tables = pd.read_csv(root_git+'balanced_datasets/unbalanced_datasets/test_unbalanced_not_augmented.csv')\n",
    "valid_tables = pd.read_csv(root_git+'balanced_datasets/unbalanced_datasets/valid_unbalanced_not_augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tables_set = get_table_set(train_tables)\n",
    "test_tables_set = get_table_set(test_tables)\n",
    "valid_tables_set = get_table_set(valid_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_no_repetitions(train_tables_set, test_tables_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_no_repetitions(train_tables_set, valid_tables_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_no_repetitions(valid_tables_set, test_tables_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root_git+'balanced_datasets/data_augementation_samples/train_tables_set.pkl', 'wb') as f:\n",
    "    pickle.dump(train_tables_set,f)\n",
    "\n",
    "with open(root_git+'balanced_datasets/data_augementation_samples/test_tables_set.pkl', 'wb') as f:\n",
    "    pickle.dump(test_tables_set,f)\n",
    "\n",
    "with open(root_git+'balanced_datasets/data_augementation_samples/valid_tables_set.pkl', 'wb') as f:\n",
    "    pickle.dump(valid_tables_set,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root_git+'balanced_datasets/data_augementation_samples/train_tables_set.pkl', 'rb') as f:\n",
    "    train_tables_set = pickle.load(f)\n",
    "\n",
    "with open(root_git+'balanced_datasets/data_augementation_samples/test_tables_set.pkl', 'rb') as f:\n",
    "    test_tables_set = pickle.load(f)\n",
    "\n",
    "with open(root_git+'balanced_datasets/data_augementation_samples/valid_tables_set.pkl', 'rb') as f:\n",
    "    valid_tables_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unbalanced = pd.read_csv(root_git+'balanced_datasets/unbalanced_datasets/train_unbalanced.csv')\n",
    "#train_unbalanced = train_unbalanced.drop('total_time', axis=1)\n",
    "train_unbalanced['a%'] = train_unbalanced['a%'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(train_unbalanced, 'a%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutting columns that are too high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = drop_from_interval(train_unbalanced, 0.5, 0.6, 363_117)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3 = drop_from_interval(train_2, 0.6, 0.7, 20_470)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_4, dropped_78 = drop_from_interval(train_3, 0.7, 0.8, 18_044) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_5, dropped_79 = drop_from_interval(train_4, 0.8, 0.9, 3_821) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_6 = drop_from_interval(train_5, 0.9, 1, 186_098)[0]\n",
    "plot_data_distribution(train_6, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_6.to_csv(root_git+'balanced_datasets/unbalanced_datasets/train_unbalanced_not_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "augm_train_1 = pd.read_csv(root_git+'balanced_datasets/data_augementation_samples/train_stats_1000000.csv')\n",
    "augm_train_2 = pd.read_csv(root_git+'balanced_datasets/data_augementation_samples/train_stats_2000000.csv')\n",
    "augm_train = pd.concat([augm_train_1, augm_train_2], axis=0)\n",
    "augm_train = augm_train.drop('total_time', axis=1)\n",
    "augm_train = augm_train.drop('id', axis=1)\n",
    "augm_train['a%'] = augm_train['a%'].fillna(0)\n",
    "augm_train = drop_duplicate_samples(train_6, augm_train)\n",
    "augm_train.to_csv(root_git+'balanced_datasets/data_augementation_samples/train_stats_concat.csv', index=False)\n",
    "augm_train = pd.read_csv(root_git+'balanced_datasets/data_augementation_samples/train_stats_concat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(augm_train, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmented = augment_interval(train_6, augm_train, 0,0.1,2_959)\n",
    "train_augmented = augment_interval(train_augmented, augm_train, 0.1, 0.2, 17_619)\n",
    "train_augmented = augment_interval(train_augmented, augm_train, 0.2, 0.3, 26_631)\n",
    "train_augmented = augment_interval(train_augmented, augm_train, 0.3, 0.4, 36_845)\n",
    "train_augmented = augment_interval(train_augmented, augm_train, 0.4, 0.5, 7_496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(train_augmented, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmented.to_csv(root_git+'balanced_datasets/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing broken triples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_filtered = pd.read_csv(root_git+'armadillo_corrections/train_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(train_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation = pd.read_csv(root_git+'armadillo_corrections/train_stats_data_augmentation_seeds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(train_data_for_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation = drop_duplicate_samples(train_data_filtered, train_data_for_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation = train_data_for_augmentation[train_data_for_augmentation['seeds']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(train_data_for_augmentation['a%']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation = train_data_for_augmentation[pd.isna(train_data_for_augmentation['a%'])]\n",
    "train_data_for_augmentation['a%'] = train_data_for_augmentation['a%'].fillna(0)\n",
    "train_data_for_augmentation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation['seeds'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_augmentation = train_data_for_augmentation.drop(['id', 'seeds'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_augmented = augment_interval(train_data_filtered, train_data_for_augmentation, 0, 0.1, 2_657)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(train_data_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(train_data_augmented['a%']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_augmented.to_csv(root_git+'armadillo_corrections/train_filtered_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building training dataset without perfect matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ones = pd.read_csv(root_git+'armadillo_corrections/train_filtered_augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_ones = remove_exact_matches(df_with_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(df_no_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augm = pd.read_csv(root_git+'armadillo_corrections/train_stats_banding_seeds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(data_augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augm = remove_exact_matches(data_augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augm = data_augm[data_augm['a%']>=0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(data_augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augm = drop_duplicate_samples(df_no_ones, data_augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augm = data_augm.drop(columns=['id', 'seeds','total_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_ones_balanced = augment_interval(df_no_ones, data_augm, 0.9, 1, 36_368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(df_no_ones_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_ones_balanced.to_csv(root_git+'train_no_perfect_matches.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unbalanced = pd.read_csv(root_git+'balanced_datasets/unbalanced_datasets/test_unbalanced.csv')\n",
    "test_unbalanced = test_unbalanced.drop('total_time', axis=1)\n",
    "test_unbalanced['a%'] = test_unbalanced['a%'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_unbalanced, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = drop_from_interval(test_unbalanced, 0, 0.1, 21_590)[0]\n",
    "test_2 = drop_from_interval(test_2, 0.1, 0.2, 7_443)[0]\n",
    "test_2 = drop_from_interval(test_2, 0.2, 0.3, 2_937)[0]\n",
    "test_2 = drop_from_interval(test_2, 0.4, 0.5, 11_163)[0]\n",
    "test_3 = drop_from_interval(test_2, 0.5, 0.6, 228_664)[0]\n",
    "test_4 = drop_from_interval(test_3, 0.6, 0.7, 23_636)[0]\n",
    "test_5 = drop_from_interval(test_4, 0.7, 0.8, 20_561)[0]\n",
    "test_6 = drop_from_interval(test_5, 0.8, 0.9, 14_011)[0]\n",
    "test_6 = drop_from_interval(test_6, 0.9, 1, 103_225)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_6, 'a%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augm_test= pd.read_csv(root_git+'balanced_datasets/data_augementation_samples/test_stats_500000.csv')\n",
    "augm_test = augm_test.drop('total_time', axis=1)\n",
    "augm_test = augm_test.drop('id', axis=1)\n",
    "augm_test['a%'] = augm_test['a%'].fillna(0)\n",
    "augm_test = drop_duplicate_samples(test_6, augm_test)\n",
    "augm_test.to_csv(root_git+'balanced_datasets/data_augementation_samples/test_stats_cleaned.csv', index=False)\n",
    "augm_test = pd.read_csv(root_git+'balanced_datasets/data_augementation_samples/test_stats_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(augm_test, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_augmented = augment_interval(test_6, augm_test, 0.3,0.4, 2_790)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_augmented, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_augmented.to_csv(root_git+'balanced_datasets/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = pd.read_csv(root_git+'balanced_datasets/test.csv')\n",
    "data_augm = pd.read_csv(root_git+'balanced_datasets/data_augementation_samples/test_stats_500000.csv')\n",
    "data_augm['a%'] = data_augm['a%'].fillna(0)\n",
    "d_0 = data_augm[data_augm['a%']==0]\n",
    "t_check = d_0.iloc[-2][1:4]\n",
    "for r in range(tg.shape[0]):\n",
    "    if ((tg.iloc[r]['r_id']==t_check['r_id']) and (tg.iloc[r]['s_id']==t_check['s_id'])) or ((tg.iloc[r]['r_id']==t_check['s_id']) and (tg.iloc[r]['s_id']==t_check['r_id'])):\n",
    "        print('nope')\n",
    "        break\n",
    "print('ok')\n",
    "tg.iloc[92616] = d_0.iloc[-2][1:4]\n",
    "tg.to_csv(root_git+'balanced_datasets/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing broken triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_filtered = pd.read_csv(root_git+'armadillo_corrections/test_filtered.csv')\n",
    "test_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_for_augmentation = pd.read_csv(root_git+'armadillo_corrections/test_stats_data_augmentation_seeds.csv')\n",
    "plot_data_distribution(test_data_for_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_for_augmentation = drop_duplicate_samples(test_data_filtered, test_data_for_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_for_augmentation = test_data_for_augmentation[test_data_for_augmentation['seeds']==0]\n",
    "test_data_for_augmentation = test_data_for_augmentation[pd.isna(test_data_for_augmentation['a%'])]\n",
    "test_data_for_augmentation['a%'] = test_data_for_augmentation['a%'].fillna(0)\n",
    "test_data_for_augmentation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_for_augmentation = test_data_for_augmentation.drop(['id', 'seeds'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_augmented = augment_interval(test_data_filtered, test_data_for_augmentation, 0, 0.1, 430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_data_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_augmented.iloc[99591]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_augmented.to_csv(root_git+'armadillo_corrections/test_filtered_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_unbalanced = pd.read_csv(root_git+'balanced_datasets/unbalanced_datasets/valid_unbalanced.csv')\n",
    "valid_unbalanced = valid_unbalanced.drop('total_time', axis=1)\n",
    "valid_unbalanced['a%'] = valid_unbalanced['a%'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(valid_unbalanced, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_2 = drop_from_interval(valid_unbalanced, 0, 0.1, 19_770)[0]\n",
    "valid_2 = drop_from_interval(valid_2, 0.1, 0.2, 9_556)[0]\n",
    "valid_2 = drop_from_interval(valid_2, 0.2, 0.3, 2_483)[0]\n",
    "valid_2 = drop_from_interval(valid_2, 0.4, 0.5, 12_259)[0]\n",
    "valid_3 = drop_from_interval(valid_2, 0.5, 0.6, 230_147)[0]\n",
    "valid_4 = drop_from_interval(valid_3, 0.6, 0.7, 22_245)[0]\n",
    "valid_5 = drop_from_interval(valid_4, 0.7, 0.8, 18_914)[0]\n",
    "valid_5 = drop_from_interval(valid_5, 0.8, 0.9, 15_190)[0]\n",
    "valid_6 = drop_from_interval(valid_5, 0.9, 1, 102_066)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(valid_6, 'a%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augm_valid = pd.read_csv(root_git+'balanced_datasets/data_augementation_samples/valid_stats_500000.csv')\n",
    "augm_valid = augm_valid.drop('total_time', axis=1)\n",
    "augm_valid = augm_valid.drop('id', axis=1)\n",
    "augm_valid['a%'] = augm_valid['a%'].fillna(0)\n",
    "augm_valid = drop_duplicate_samples(valid_6, augm_valid)\n",
    "augm_valid.to_csv(root_git+'balanced_datasets/data_augementation_samples/valid_stats_cleaned.csv', index=False)\n",
    "augm_valid = pd.read_csv(root_git+'balanced_datasets/data_augementation_samples/valid_stats_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(augm_valid, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_augmented = augment_interval(valid_6, augm_valid, 0.3,0.4, 2_811)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(valid_augmented, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_augmented.to_csv(root_git+'balanced_datasets/valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing broken triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_filtered = pd.read_csv(root_git+'armadillo_corrections/valid_filtered.csv')\n",
    "valid_data_for_augmentation = pd.read_csv(root_git+'armadillo_corrections/valid_stats_data_augmentation_seeds.csv')\n",
    "valid_data_for_augmentation = drop_duplicate_samples(valid_data_filtered, valid_data_for_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_for_augmentation = valid_data_for_augmentation[valid_data_for_augmentation['seeds']==0]\n",
    "valid_data_for_augmentation = valid_data_for_augmentation[pd.isna(valid_data_for_augmentation['a%'])]\n",
    "valid_data_for_augmentation['a%'] = valid_data_for_augmentation['a%'].fillna(0)\n",
    "valid_data_for_augmentation.shape\n",
    "valid_data_for_augmentation = valid_data_for_augmentation.drop(['id', 'seeds'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(valid_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_augmented = augment_interval(valid_data_filtered, valid_data_for_augmentation, 0, 0.1, 377)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(valid_data_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_augmented.to_csv(root_git+'armadillo_corrections/valid_filtered_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(root_git+'armadillo_corrections/train_filtered_augmented.csv')\n",
    "test = pd.read_csv(root_git+'armadillo_corrections/test_filtered_augmented.csv')\n",
    "valid = pd.read_csv(root_git+'armadillo_corrections/valid_filtered_augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "def compute_counts(t: pd.DataFrame) -> tuple:\n",
    "    value_count = {}\n",
    "    n_nans = 0\n",
    "    n_str = 0\n",
    "    n_num = 0\n",
    "    n_bools = 0\n",
    "\n",
    "    for r in range(t.shape[0]):\n",
    "        for c in range(t.shape[1]):\n",
    "            v = t.iloc[r][c]\n",
    "            value_count[v] = 0\n",
    "            if pd.isna(v):\n",
    "                n_nans += 1\n",
    "            elif isinstance(v, bool):\n",
    "                n_bools += 1\n",
    "            elif isinstance(v, numbers.Number):\n",
    "                n_num += 1\n",
    "            else:\n",
    "                n_str += 1\n",
    "    return len(value_count.keys()), n_num, n_str, n_nans, n_bools\n",
    "\n",
    "def compute_table_stats(table_dict: dict | str, outpath: str) -> None:\n",
    "    if isinstance(table_dict, str):\n",
    "        with open(table_dict, 'rb') as f:\n",
    "            table_dict = pickle.load(f)\n",
    "    new_cols = {\n",
    "        'table_name':[],\n",
    "        'n_cols':[],\n",
    "        'n_rows':[],\n",
    "        'area':[],\n",
    "        'n_distinct_values':[],\n",
    "        'n_numerical':[],\n",
    "        'n_textual':[],\n",
    "        'n_nans':[],\n",
    "        'n_bools':[]\n",
    "    }\n",
    "\n",
    "    for k in tqdm(table_dict.keys()):\n",
    "        t = table_dict[k]\n",
    "        rows = t.shape[0]\n",
    "        cols = t.shape[1]\n",
    "        area = rows * cols\n",
    "        n_distinct_values, n_numerical, n_textual, n_nans, n_bools = compute_counts(t)\n",
    "        new_cols['table_name'].append(k)\n",
    "        new_cols['n_cols'].append(cols)\n",
    "        new_cols['n_rows'].append(rows)\n",
    "        new_cols['area'].append(area)\n",
    "        new_cols['n_distinct_values'].append(n_distinct_values)\n",
    "        new_cols['n_numerical'].append(n_numerical)\n",
    "        new_cols['n_textual'].append(n_textual)\n",
    "        new_cols['n_nans'].append(n_nans)\n",
    "        new_cols['n_bools'].append(n_bools)\n",
    "    \n",
    "    out = pd.DataFrame(new_cols)\n",
    "    out.to_csv(outpath)\n",
    "    print(out.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root_git+'table_dict_796970_good.pkl','rb') as f:\n",
    "    table_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_stats(table_dict, root_git+'gittables_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag_train = get_table_set(pd.read_csv(root_git+'balanced_datasets/train.csv'))\n",
    "# ag_test = get_table_set(pd.read_csv(root_git+'balanced_datasets/test.csv'))\n",
    "# ag_valid = get_table_set(pd.read_csv(root_git+'balanced_datasets/valid.csv'))\n",
    "ag_train = get_table_set(train)\n",
    "ag_test = get_table_set(test)\n",
    "ag_valid = get_table_set(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_no_repetitions(ag_train, ag_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_no_repetitions(ag_train, ag_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_no_repetitions(ag_valid, ag_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikitables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(root_wiki+'/lsh_candidates/train_stats_2000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['total_time'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(root_wiki+'/lsh_candidates/train_stats_2000000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb = pd.read_csv(root_wiki+'/lsh_candidates/high_similarity/train_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb['a%'] = train_nb['a%'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(train_nb, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb = drop_from_interval(train_nb, 0,0.1, 93_636)[0]\n",
    "train_nb = drop_from_interval(train_nb, 0.1,0.2, 173_023)[0]\n",
    "train_nb = drop_from_interval(train_nb, 0.2,0.3, 305_233)[0]\n",
    "train_nb = drop_from_interval(train_nb, 0.3,0.4, 144_848)[0]\n",
    "train_nb = drop_from_interval(train_nb, 0.4,0.5, 75_015)[0]\n",
    "train_nb = drop_from_interval(train_nb, 0.5,0.6, 127_137)[0]\n",
    "train_nb = drop_from_interval(train_nb, 0.6,0.7, 20_342)[0]\n",
    "train_nb = drop_from_interval(train_nb, 0.7,0.8, 4_713)[0]\n",
    "train_nb = drop_from_interval(train_nb, 0.8,0.9, 48_002)[0]\n",
    "train_nb = drop_from_interval(train_nb, 0.9,1, 212_411)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(train_nb, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb = train_nb.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb.to_csv(root_wiki+'/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(root_wiki+'/lsh_candidates/test_stats_750000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(root_wiki+'/lsh_candidates/test_stats_750000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_augm = test_df[test_df['a%'] >= 0.6]\n",
    "test_augm = test_augm[test_augm['a%'] < 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_augm, 'a%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nb = pd.read_csv(root_wiki+'/lsh_candidates/high_similarity/test_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_nb, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nb['a%'] = test_nb['a%'].fillna(0)\n",
    "test_nb = drop_from_interval(test_nb, 0,0.1,10_953)[0]\n",
    "test_nb = drop_from_interval(test_nb, 0.1,0.2,16_629)[0]\n",
    "test_nb = drop_from_interval(test_nb, 0.2,0.3,30_013)[0]\n",
    "test_nb = drop_from_interval(test_nb, 0.3,0.4,14_674)[0]\n",
    "test_nb = drop_from_interval(test_nb, 0.4,0.5,6_923)[0]\n",
    "test_nb = drop_from_interval(test_nb, 0.5,0.6,13_537)[0]\n",
    "test_nb = drop_from_interval(test_nb, 0.6,0.7,1_753)[0]\n",
    "test_nb = drop_from_interval(test_nb, 0.7,0.8,437)[0]\n",
    "test_nb = drop_from_interval(test_nb, 0.8,0.9,5_208)[0]\n",
    "test_nb = drop_from_interval(test_nb, 0.9,1,23_495)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_nb, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nb = test_nb.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nb.to_csv(root_wiki+'/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(root_wiki+'/lsh_candidates/valid_stats_750000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(root_wiki+'/lsh_candidates/valid_stats_750000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(root_wiki+'/lsh_candidates/high_similarity/valid_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_nb = pd.read_csv(root_wiki+'/lsh_candidates/high_similarity/valid_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(valid_nb, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_nb['a%'] = valid_nb['a%'].fillna(0)\n",
    "valid_nb = drop_from_interval(valid_nb, 0, 0.1, 8_687)[0]\n",
    "valid_nb = drop_from_interval(valid_nb, 0.1, 0.2, 19_913)[0]\n",
    "valid_nb = drop_from_interval(valid_nb, 0.2, 0.3, 36_107)[0]\n",
    "valid_nb = drop_from_interval(valid_nb, 0.3, 0.4, 16_003)[0]\n",
    "valid_nb = drop_from_interval(valid_nb, 0.4, 0.5, 8_005)[0]\n",
    "valid_nb = drop_from_interval(valid_nb, 0.5, 0.6, 12_964)[0]\n",
    "valid_nb = drop_from_interval(valid_nb, 0.6, 0.7, 793)[0]\n",
    "valid_nb = drop_from_interval(valid_nb, 0.7, 0.8, 102)[0]\n",
    "valid_nb = drop_from_interval(valid_nb, 0.8, 0.9, 4_942)[0]\n",
    "valid_nb = drop_from_interval(valid_nb, 0.9, 1, 21_038)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(valid_nb, 'a%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_nb = valid_nb.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_nb.to_csv(root_wiki+'/valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New GitTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples = pd.read_csv(root_git+'/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(train_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples = drop_from_interval(train_triples, 0, 0.1, 833)[0]\n",
    "train_triples = drop_from_interval(train_triples, 0.1, 0.2, 62_676)[0]\n",
    "train_triples = drop_from_interval(train_triples, 0.2, 0.3, 11_845)[0]\n",
    "train_triples = drop_from_interval(train_triples, 0.3, 0.4, 9_826)[0]\n",
    "train_triples = drop_from_interval(train_triples, 0.4, 0.5, 28_478)[0]\n",
    "train_triples = drop_from_interval(train_triples, 0.5, 0.6, 341_304)[0]\n",
    "train_triples = drop_from_interval(train_triples, 0.6, 0.7, 73_939)[0]\n",
    "train_triples = drop_from_interval(train_triples, 0.7, 0.8, 66_192)[0]\n",
    "train_triples = drop_from_interval(train_triples, 0.8, 0.9, 440)[0]\n",
    "train_triples = drop_from_interval(train_triples, 0.9, 1, 497_191)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(train_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_triples.to_csv(root_git+'/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triples = pd.read_csv(root_git+'/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triples = drop_from_interval(test_triples, 0, 0.1, 13_795)[0]\n",
    "test_triples = drop_from_interval(test_triples, 0.1, 0.2, 39_623)[0]\n",
    "test_triples = drop_from_interval(test_triples, 0.2, 0.3, 9_786)[0]\n",
    "test_triples = drop_from_interval(test_triples, 0.3, 0.4, 4_250)[0]\n",
    "test_triples = drop_from_interval(test_triples, 0.4, 0.5, 16_937)[0]\n",
    "test_triples = drop_from_interval(test_triples, 0.5, 0.6, 219_120)[0]\n",
    "test_triples = drop_from_interval(test_triples, 0.6, 0.7, 33_127)[0]\n",
    "test_triples = drop_from_interval(test_triples, 0.7, 0.8, 25_157)[0]\n",
    "test_triples = drop_from_interval(test_triples, 0.8, 0.9, 509)[0]\n",
    "test_triples = drop_from_interval(test_triples, 0.9, 1, 166547)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(test_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_triples.to_csv(root_git+'/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_triples = pd.read_csv(root_git+'/valid.csv')\n",
    "plot_data_distribution(valid_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_triples = drop_from_interval(valid_triples, 0, 0.1, 4_134)[0]\n",
    "valid_triples = drop_from_interval(valid_triples, 0.1, 0.2, 23_875)[0]\n",
    "valid_triples = drop_from_interval(valid_triples, 0.2, 0.3, 5_013)[0]\n",
    "valid_triples = drop_from_interval(valid_triples, 0.3, 0.4, 1_960)[0]\n",
    "valid_triples = drop_from_interval(valid_triples, 0.4, 0.5, 12_112)[0]\n",
    "valid_triples = drop_from_interval(valid_triples, 0.5, 0.6, 123_963)[0]\n",
    "valid_triples = drop_from_interval(valid_triples, 0.6, 0.7, 19_992)[0]\n",
    "valid_triples = drop_from_interval(valid_triples, 0.7, 0.8, 16_206)[0]\n",
    "valid_triples = drop_from_interval(valid_triples, 0.8, 0.9, 198)[0]\n",
    "valid_triples = drop_from_interval(valid_triples, 0.9, 1, 148_751)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(valid_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_triples.to_csv(root_git+'/valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stats_git = pd.read_csv(root_git+'/table_stats/stats.csv')\n",
    "print(stats_git.shape)\n",
    "stats_git.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stats_wiki = pd.read_csv(root_wiki+'/table_stats/stats.csv')\n",
    "print(stats_wiki.shape)\n",
    "stats_wiki.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_wiki = pd.read_csv(root_wiki+'/table_stats/stats.csv')\n",
    "stats_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_wiki.drop(index=[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root_wiki+'/dictionaries/embedding_dictionaries/t_execs_armadillo_w_w.pkl','rb') as f:\n",
    "    dd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(root_wiki+'/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tables = set(dd.keys())\n",
    "missing = []\n",
    "for r in tqdm(range(df.shape[0])):\n",
    "    r_id = df.iloc[r].loc['r_id']\n",
    "    s_id = df.iloc[r].loc['s_id']\n",
    "    if r_id not in tables:\n",
    "        missing.append(r_id)\n",
    "    if s_id not in tables:\n",
    "        missing.append(s_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(root_git+'/table_querying/evaluation/table_querying_results.csv') \n",
    "df.iloc[:,13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bert_r_overlap_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv(root_git+'/table_querying/evaluation/table_querying_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['armadillo_g_g_ae'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_list(df):\n",
    "    out = []\n",
    "    for r in range(df.shape[0]):\n",
    "        for c in range(df.shape[1]):\n",
    "            out.append(df.iloc[r].iloc[c])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_list(df):\n",
    "    df.values\n",
    "    out = []\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_value_list(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.iloc[:50, :10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_d = dd.iloc[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_value_list(tmp_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_value_list(df):\n",
    "    values = df.values.tolist()\n",
    "    out = list(itertools.chain.from_iterable(values))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_value_list(dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_d.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for l in tmp_d.values.tolist():\n",
    "    out+=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNNTE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
